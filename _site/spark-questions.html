<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>『 Spark 』2. spark 基本概念解析 | Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
  html {
    background: #333333;
    -webkit-background-size: cover;
    -moz-background-size: cover;
    -o-background-size: cover;
    background-size: cover;
  }
  /*body { background:transparent;}*/
  @media screen and (max-width: 750px){
    body { background: rgba(255, 255, 255, 0.9); }
  }
</style>

<div id="content" class="post" style="margin-top: 20px;">
  <div id="avatar" class="avatar circle" data-in-right="false" style="width: 150px; height: 150px; position: fixed; top: 40px; z-index: 99; opacity: 0;">
    <div class="center" style="margin-top: 4px; height: 142px; width: 142px; border-radius: 71px; background-image: url('../images/2.jpg');"></div>
  </div>

  <div class="entry" style="position: relative;">
    <h1 class="entry-title"><a href="/spark-questions" title="『 Spark 』2. spark 基本概念解析">『 Spark 』2. spark 基本概念解析</a></h1>

    <p class="entry-date">2016-02-05 <span class="lastModified" style="display: none;" data-source="_posts/new-spark/2016-02-05-spark-questions.md">最后更新时间: </span></p>


    <h2 id="section">写在前面</h2>

<p>本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，并非为了做什么教程，所以一切以个人理解梳理为主，没有必要的细节就不会记录了。若想深入了解，最好阅读参考文章和官方文档。</p>

<p>其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。</p>

<h2 id="application">1. Application</h2>

<p>用户在 spark 上构建的程序，包含了 driver 程序以及集群上的 executors.</p>

<h2 id="driver-program">2. Driver Program</h2>

<p>运行 main 函数并且创建 SparkContext 的程序。</p>

<h2 id="cluster-manager">3. Cluster Manager</h2>

<p>集群的资源管理器，在集群上获取资源的外部服务。
拿 Yarn 举例，客户端程序会向 Yarn 申请计算我这个任务需要多少的 memory，多少 CPU，etc。
然后 Cluster Manager 会通过调度告诉客户端可以使用，然后客户端就可以把程序送到每个 Worker Node 上面去执行了。</p>

<h2 id="worker-node">4. Worker Node</h2>

<p>集群中任何一个可以运行spark应用代码的节点。
Worker Node就是物理节点，可以在上面启动Executor进程。</p>

<h2 id="executor">5. Executor</h2>

<p>在每个 Worker Node 上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上，每个任务都有各自独立的 Executor。
Executor 是一个执行 Task 的容器。它的主要职责是：</p>

<p>1、初始化程序要执行的上下文 SparkEnv，解决应用程序需要运行时的 jar 包的依赖，加载类。
2、同时还有一个 ExecutorBackend 向 cluster manager 汇报当前的任务状态，这一方面有点类似 hadoop的 tasktracker 和 task。</p>

<p>总结：Executor 是一个应用程序运行的监控和执行容器。</p>

<h2 id="jobs">6. Jobs</h2>

<p>包含很多 task 的并行计算，可以认为是 Spark RDD 里面的 action,每个 action 的计算会生成一个job。
用户提交的 Job 会提交给 DAGScheduler，Job 会被分解成 Stage和Task。</p>

<p><img src="../images/spark-web-ui-job.jpg" alt="spark-web-ui-job.jpg" /></p>

<p>A job is triggered by an <code class="highlighter-rouge">action</code>, like <code class="highlighter-rouge">count()</code> or <code class="highlighter-rouge">saveAsTextFile()</code>, click on a job to see info about the <code class="highlighter-rouge">stages</code> of <code class="highlighter-rouge">tasks</code> inside it.</p>

<h2 id="stage">7. Stage</h2>

<p>一个 Job 会被拆分为多组 Task，每组任务被称为一个 Stage就像Map Stage， Reduce Stage。</p>

<p>Stage 的划分在 RDD 的论文中有详细的介绍，简单的说是以 shuffle和result 这两种类型来划分。
在 Spark 中有两类 task:</p>

<ul>
  <li>
    <p>shuffleMapTask</p>

    <p>输出是shuffle所需数据, stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。</p>
  </li>
  <li>
    <p>resultTask，</p>

    <p>输出是result，比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。</p>
  </li>
</ul>

<h2 id="task">8. Task</h2>

<p>被送到executor上的工作单元，Spark上分为2类task。</p>

<ul>
  <li>
    <p>shuffleMapTask</p>

    <p>A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner specified in the ShuffleDependency).</p>
  </li>
  <li>
    <p>resultTask</p>

    <p>A task that sends back the output to the driver application.</p>
  </li>
</ul>

<h2 id="partition">9. Partition</h2>

<p>Partition类似hadoop的Split，计算是以partition为单位进行的，当然partition的划分依据有很多，这是可以自己定义的，像HDFS文件，划分的方式就和MapReduce一样，以文件的block来划分不同的partition。总而言之，Spark的partition在概念上与hadoop中的split是相似的，提供了一种划分数据的方式。</p>

<h2 id="rdd">10. RDD</h2>

<p>先看看原文 <a href="../files/spark-rdd-paper.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing</a> 是怎么介绍 RDD 的。</p>

<hr />

<p>a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner.</p>

<p>RDDs are motivated by two types of applications that current computing frameworks handle inefficiently:</p>

<ul>
  <li>iterative algorithms;</li>
  <li>interactive data mining tools;</li>
</ul>

<p>In both cases, keeping data in memory can improve performance by an order of magnitude.</p>

<p>To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained
transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.</p>

<hr />

<p>每个RDD有5个主要的属性：</p>

<ul>
  <li>一组分片（partition），即数据集的基本组成单位</li>
  <li>一个计算每个分片的函数</li>
  <li>对parent RDD的依赖，这个依赖描述了RDD之间的lineage</li>
  <li>对于key-value的RDD，一个Partitioner，这是可选择的</li>
  <li>一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的</li>
</ul>

<p>　　把上面这5个主要的属性总结一下，可以得出RDD的大致概念。首先要知道，RDD大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是spark项目组设计用来表示数据集的一种数据结构。而spark项目组为了让RDD能handle更多的问题，又规定RDD应该是只读的，分区记录的一种数据集合中。可以通过两种方式来创建RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建RDD的方式，即通过其他RDD来创建【以后叫做转换】而成。而正因为RDD满足了这么多特性，所以spark把RDD叫做Resilient Distributed Datasets，中文叫做弹性分布式数据集。很多文章都是先讲RDD的定义，概念，再来说RDD的特性。我觉得其实也可以倒过来，通过RDD的特性反过来理解RDD的定义和概念，通过这种由果溯因的方式来理解RDD也未尝不可。反正对我个人而言这种方式是挺好的。</p>

<p>RDD是Spark的核心，也是整个Spark的架构基础，可以总下出几个它的特性来：</p>

<ul>
  <li>它是不变的数据结构存储</li>
  <li>它是支持跨集群的分布式数据结构</li>
  <li>可以根据数据记录的key对结构进行分区</li>
  <li>提供了粗粒度的操作，且这些操作都支持分区</li>
  <li>它将数据存储在内存中，从而提供了低延迟性</li>
</ul>

<h2 id="scparallelize">11. sc.parallelize</h2>

<p>先看看 api 文档里是怎么说的：<a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize">parallelize</a></p>

<hr />

<ul>
  <li>parallelize(c, numSlices=None)</li>
</ul>

<p>Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance.</p>

<hr />

<h2 id="code-distribute">12. code distribute</h2>

<p>提交 spark 应用时，spark 会把应用代码分发到所有的 worker 上面，应用依赖的包需要在所有的worker上都存在，有两种解决 worker 上相关包依赖的问题：</p>

<ul>
  <li>选用一些工具统一部署 spark cluster；</li>
  <li>在提交 spark 应用的时候，指定应用依赖的相关包，把 应用代码，应用依赖包 一起分发到 worker；</li>
</ul>

<h2 id="cache-priority">13. cache priority</h2>

<p>cache 是否支持 priority，目前不支持，而且 spark 里面对 rdd 的 cache 和我们常见的缓存系统是不一样的。细节可以找我讨论。</p>

<h2 id="cores">14. cores</h2>

<p>The number of cores to use on each executor. For YARN and standalone mode only. In standalone mode, setting this parameter allows an application to run multiple executors on the same worker, provided that there are enough cores on that worker. Otherwise, only one executor per application will run on each worker.</p>

<p>每一个 core，相当于一个 worker 上的进程，这些进程会同时执行分配到这个 worker 上的任务。简单的说，就是 spark manager 把一个 job 切分几个 task 分发到 worker 上同步执行，而每个 worker 把分配给自己的 task 再切分成几个 subtask，分配给当前 worker 上的不同进程。</p>

<h2 id="memory">15. Memory</h2>

<p>分配给 spark 应用的内存是仅仅给 cache 数据用吗？</p>

<p>这个问题目前没有查看得非常仔细，后续继续更新。</p>

<h2 id="rdd-narrowwide-dependences">16. RDD narrow/wide dependences</h2>

<p>RDD 之间的依赖类别［ 或者，创建一个 RDD 的不同方法 ］</p>

<p><img src="../images/rdd-dependences.jpg" alt="rdd-dependences.jpg" /></p>

<h2 id="section-1">17. 本地内存与集群内存</h2>

<p>所谓本地内存，是指在 driver 端的程序所需要的内存，由 driver 机器提供，一般用来生成测试数据，接受运算结果等；
所谓集群内存，是指提交到集群的作业能够向集群申请的最多内存使用量，一般用来存储关键数据；</p>

<p><img src="../images/spark-memory-cluster-and-driver.jpg" alt="spark-memory-cluster-and-driver.jpg" /></p>

<h2 id="section-2">18. 限制用户使用的内存</h2>

<p>可以在启动 spark 应用的时候申请；完全可控。</p>

<h2 id="section-3">19. 当用户申请总资源超过当前集群总资源</h2>

<p>FIFO 资源分配方式。</p>

<h2 id="next">4. Next</h2>

<p>下一篇，通过几个简单的例子来介绍 spark 的基本编程模式。</p>

<h2 id="section-4">参考文章</h2>

<ul>
  <li><a href="../files/spark-rdd-paper.pdf">spark-rdd-paper : Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing</a></li>
  <li><a href="http://spark.apache.org/docs/latest/api/python/pyspark.html">spark python API</a></li>
</ul>

<h2 id="section-5">本系列文章链接</h2>

<ul>
  <li><a href="../introduction-to-spark">『 Spark 』1. spark 简介 </a></li>
  <li><a href="../spark-questions-concepts">『 Spark 』2. spark 基本概念解析 </a></li>
</ul>



    <!-- share icon -->
    <div class="ds-share" data-thread-key="/spark-questions" data-title="『 Spark 』2. spark 基本概念解析"
         data-content="content"
         data-url="http://litaotao.github.io//spark-questions">
        <div class="ds-share-aside-left">
          <div class="ds-share-aside-inner">
          </div>
          <div class="ds-share-aside-toggle">分享</div>
        </div>
    </div>

    <div id="disqus_container">
      <div style="margin-bottom:20px">
      <!-- 多说评论框 start -->
        <div class="ds-thread" data-thread-key=/spark-questions data-title=『 Spark 』2. spark 基本概念解析 data-url=/spark-questions></div>
      <!-- 多说评论框 end -->
      <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"litaotao"};
        (function() {
          var ds = document.createElement('script');
          ds.type = 'text/javascript';ds.async = true;
          ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
          ds.charset = 'UTF-8';
          (document.getElementsByTagName('head')[0]
           || document.getElementsByTagName('body')[0]).appendChild(ds);
        })();
        </script>
      <!-- 多说公共JS代码 end -->
      </div>
    </div>
  </div>

  <div id="menuIndex" class="sidenav">
    <div class="myinfo"><center>
      <div id="avatarHolder" class="avatar circle" style="width: 0px; height: 0px; box-shadow: none; margin-bottom: 20px;"></div>
      <a href="/index.html" title="Homepage"><i class="icon-home icon-large"></i> Home</a>
      <a href="http://www.linkedin.com/in/taotaoli"><i class="icon-linkedin-sign icon-large"></i><span> Profile</span></a>
      <a href="https://github.com/litaotao"><i class="icon-github icon-large"></i><span> Code</span></a>
      <a href="mailto:taotao.engineer@gmail.com"><i class="icon-envelope-alt icon-large"></i><span> Mail</span></a>
    </center></div>
    <div id="menu"></div>
  </div>
</div>


<script src="/js/post.js" type="text/javascript"></script>

</body>
</html>
